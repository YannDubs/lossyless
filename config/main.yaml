defaults:

  # GENERAL #
  - hydra/job_logging: colorlog
  - hydra/hydra_logging: colorlog

  - _self_
  - user
  
  - logger: wandb
  - server: local  

  # FEATURIZER #
  - featurizer: neural_rec
  - distortion: ivae
  - rate: H_factorized
  - data@datafeat: mnist
  - architecture@featurizer: resnet
  - checkpoint@checkpoint_feat: bestValLoss # NB : will not be able to use Val loss for vib because validation is meaningless for distortion
  - optimizer@optimizer_compressor: adam1e-3
  - scheduler@scheduler_compressor: expdecay
  - optimizer@optimizer_coder: adam3e-3
  - scheduler@scheduler_coder: expdecay

  # PREDICTOR #
  - architecture@predictor: resnet18
  - data@datapred: mnist
  - architecture@predictor: resnet18
  - checkpoint@checkpoint_pred: bestValLoss 
  - optimizer@optimizer_pred: adam1e-3
  - scheduler@scheduler_pred: expdecay


########## GENERAL ##########
experiment: ???
job_id: ??? # unique identifier
seed: 123
timeout: 1440 # 24 hours
time: ${hydra:job.num}_${now:%Y-%m-%d_%H-%M-%S} # add job num because tiem is not when job runs
long_name_feat: exp_${experiment}/datafeat_${datafeat.name}/feat_${featurizer.name}/dist_${distortion.name}/enc_${encoder.name}/rate_${rate.name}/optfeat_${optimizer_compressor.name}/schedfeat_${scheduler_featurizer.name}/zdim_${encoder.z_dim}/zs_${loss.n_z_samples}/beta_${loss.beta}/seed_${seed}/addfeat_{other.add_feat}
long_name_pred: ${long_name_feat}/datapred_${datapred.name}/optpred_${optimizer_predictor.name}/schedpred_${scheduler_predictor.name}/addpred_{other.add_pred}

paths:
  # the best practice is not to modify those paths but to simlink them to the places you want
  base_dir: ${hydra:runtime.cwd} 
  data: ${paths.base_dir}/data
  work: ${hydra.runtime.cwd}/outputs/${now:%Y-%m-%d_%H-%M-%S} # unfortunately cannot use hydra: in hydra so need to do everything by hand i.e. cannot use ${paths.base_dir}/outputs/{time}
  results: ${paths.base_dir}/results/${long_name}/jid_${job_id}
  logs: ${paths.base_dir}/logs/${long_name}/jid_${job_id}
  chckpnt: ${paths.base_dir}/checkpoints/${long_name}/jid_${job_id}
  pretrained: 
    save: ${paths.base_dir}/pretrained/${long_name}/jid_${job_id} # directory for saving pretrained models
    load: ${paths.base_dir}/pretrained/${long_name}/*  # directory for loading pretrained models if you use ** or * it will glob all matching files and take the latest

other: # some meta information that can be useful for internal stuff (usually dirty workarounds or for logging)
  is_debug: False # using debug mode
  is_quick: False # using a "quick" mode and should not log very slow things
  hydra_job_id: ${hydra:job.id} # this is the job id without the sweep number. Useful for filtering and grouping in wandb
  add_feat: null # some additional value for saving (e.g. current sweeping values)
  add_pred: null # some additional value for saving (e.g. current sweeping values)

### STAGE SPECIFIC ###
stage: ??? 
long_name: ???
data: {}
checkpoint: {}

### RUNNING ###
evaluation:
  is_est_entropies: False # whether to evaluate sample estimates of the entropies H[Y|Z], H[M(X)|Z], H[Z] should only use it with deterministic Z
  featurizer:
    ckpt_path: "best"
    is_evaluate: ${featurizer.is_train}
  predictor:
    ckpt_path: "best"
    is_evaluate: ${predictor.is_train}

callbacks: # callbacks for feat and pred
  additional: [LearningRateMonitor] # can use any callback name of lossyless.calllbacks, pl_bolts.callbacks, pl.callbacks
  # all callback kwargs should be here (loaded automatically if name corresponds)
  LearningRateMonitor:
    logging_interval : epoch

trainer:
  #default_root_dir: ${paths.results}
  max_epochs: 200
  terminate_on_nan: false # makes it slower if true
  progress_bar_refresh_rate: 0 # increase to show progress bar
  resume_from_checkpoint: null # string to checkpoint if want to resume
  gradient_clip_val: 3 # 0 means no clip #TODO tune (1 seemed to be good in compressai)
  reload_dataloaders_every_epoch: False
  log_every_n_steps: 100

  # ENGINEERING / SPEED #
  gpus: 1 
  num_nodes: 1  # number gpu nodes
  precision: 16 # use 16 bit for speed 

  # DEBUGGING #
  fast_dev_run: false # use true to make a quick test (not full epoch)
  track_grad_norm: -1 # use 2 to track L2 norms of grad
  overfit_batches: 0.0 # use 0.01 to make sure you can overfit 1% of training data => training works
  weights_summary: full # full to print show the entire model, top nly prints the top module
  profiler: null # use `simple` or `"advanced"` to find bottleneck


########## FEATURIZER ##########
### DATA ###
datafeat: {}

### MODEL ###
encoder:
  name: ???
  z_dim: 10
  arch: ???
  arch_kwargs: {}
  fam_kwargs: {}

### OPTIMIZER ###
optimizer_compressor: 
  is_lars: false # whether to use LARS optimizer, useful for large batches.
scheduler_compressor: {}

optimizer_coder: {}
scheduler_coder: {}

### RUNNING ###
checkpoint_feat: {}
# dictionnary that will update the trainer (this is done because often the trainer is the same for feat and pred so want to minimize replication)
update_trainer_feat: {}


########## PREDICTOR ##########
### DATA ###
datapred: {}

### MODEL ###
predictor:
  name: ???
  is_train: True 
  arch: ???
  arch_kwargs: {}

### OPTIMIZER ###
optimizer_predictor: 
  is_lr_find: True # whether to automatically look for best lr (only set the sugggested value if lr is None, if not just plots)
scheduler_predictor: {}

### RUNNING ###
checkpoint_pred: {}
# dictionnary that will update the trainer (this is done because often the trainer is the same for feat and pred so want to minimize replication)
update_trainer_pred: {}


########## HYDRA ##########
hydra:
  job:
    env_set:
      NCCL_DEBUG: INFO 

  run:
    dir: ${paths.work}

  sweep:
    dir:  ${paths.work}
    subdir: ${hydra.job.num}_${hydra.job.id}