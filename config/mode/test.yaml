# @package _global_

# test whether can overfit training

timeout: 60

other:
  is_quick: True

trainer:
  track_grad_norm: 2 # use 2 to track L2 norms of grad
  overfit_batches: 0.01 # use 0.01 to make sure you can overfit 1% of training data => training works
  max_epochs: 100
  log_every_n_steps: 20

logger:
  wandb_kwargs: 
    tags: [test, quick]
    anonymous: true
    project: tmp

callbacks:
  GPUStatsMonitor: 
    is_use : true

  TrainingDataMonitor:
    is_use : true
    kwargs:
      log_every_n_steps: 50

  ModuleDataMonitor:
    is_use : true
    kwargs:
      log_every_n_steps: 50
      submodules: ["p_ZlX.mapper"]
  