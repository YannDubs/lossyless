defaults:
  - hydra/job_logging: colorlog
  - hydra/hydra_logging: colorlog
  
  - data: miniFashionMnist
  - encoder: cnn
  - distortion: vae
  - rate: MI_unitgaussian

### GENERAL ###
name: ???
seed: 123
is_debug: False # enter debug mode
time: ${now:%Y-%m-%d_%H-%M-%S}

long_name: ${name}/data_${data.name}/d_${distortion.name}/e_${encoder.name}/r_${rate.name}/o_${optimizer.name}/dim_${encoder.z_dim}/z_${loss.n_z_samples}/b_${loss.beta}/s_${seed}/${time}

paths:
  base_dir: ???
  data: ${paths.base_dir}/data
  pretrained: ${paths.base_dir}/pretrained/${long_name}
  logs: ${paths.base_dir}/logs
  eval: ${paths.base_dir}/results/${long_name}/eval.csv

optimizer: # might need to be a group at some point
  name: adam # not used yet but can change if needed
  lr: 1e-3
  weight_decay: 0
  is_lars: false # whether to use LARS optimizer, useful for large batches.
  scheduler:
    name: "expdecay"
    decay_factor: 100 # by how much to reduce lr during training

# only used if coder needs an optimizer (for coding)
optimizer_coder :
  name: adam # not used yet but can change if needed
  lr: 1e-3
  scheduler:
    name: null


logger:
  loggers: ["csv","wandb"]

  csv:
    save_dir: ${paths.logs}/csv/${long_name}/
    name: ${time}

  wandb:
    name: ${time}
    project: lossyless
    group: ${name}
    offline: false # Run offline (data can be streamed later to wandb servers).

callbacks:
  additional: []

  ModelCheckpoint_compressor:
    filepath: ${paths.pretrained} 
    monitor: train_loss
    mode: "min"
    verbose: true

  OnlineEvaluator:
    dropout_p: 0.2
    hidden_dim: 1024
    in_dim: ${encoder.z_dim}
    y_shape: ${data.target_shape}
    is_classification: ${data.target_is_clf}

predictor:
  is_online_eval: true

trainer:
  #default_root_dir: ${paths.results}
  max_epochs: 100
  terminate_on_nan: true
  progress_bar_refresh_rate: 0 # increase to show progress bar
  resume_from_checkpoint: null # string to checkpoint if want to resume
  gradient_clip_val: 0 # increase to clip grad

  
  # ENGINEERING / SPEED
  gpus: 1 
  num_nodes: 1  # number gpu nodes
  precision: 16 # use 16 bit for speed 

  # DEBUGGING
  fast_dev_run: false # use true to make a quick test (not full epoch)
  track_grad_norm: -1 # use 2 to track L2 norms of grad
  overfit_batches: 0.0 # use 0.01 to make sure you can overfit 1% of training data => training works
  weights_summary: top # full to print show the entire model 
  profiler: null # use `simple` or `"advanced"` to find bottleneck

### GROUP DEFAULTS ###
data:
  name: ???
  dataset: ???
  length: ???
  shape: ???
  target_shape: ???
  aux_shape: ???
  aux_is_clf: ??? 
  target_is_clf: ???
  neg_factor: ??? # useful for contrastive loss. should be len(train_dataset) / (2*batch_size-1)

  kwargs:
    data_dir: ${paths.data}
    batch_size: 128

encoder:
  name: ???
  z_dim: 10
  arch: ???
  arch_kwargs:
    complexity: ???
  fam: diaggaussian
  fam_kwargs: {}

distortion:
  name: ???
  mode: ???
  factor_beta : 1 # factor that multiplies beta 
  kwargs: {}

rate:
  name: ???
  range_coder: null
  factor_beta : 1 # factor that multiplies beta 
  kwargs: {}

loss:
  n_z_samples: 1 # number of samples tu use inside log (like IWAE)
  beta: 1

### HYDRA ###