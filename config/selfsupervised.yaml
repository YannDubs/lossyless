defaults:
  - _self_
  - user
  
  - logger: wandb
  - server: none
  - hypopt: none

  - optimizer: AdamW
  - scheduler: expdecay100
  - architecture: mlp_probe
  - checkpoint: bestValLoss

  # OVERIDES #
  - override hydra/job_logging: colorlog
  - override hydra/hydra_logging: colorlog
  
experiment: ???
model: ???
dataset : ???
job_id : ???
stage: pred
is_no_save: False
monitor_return: ["test/pred/err"] 
monitor_direction: ["minimize"]
compress: none
long_name: exp_${experiment}/data_${dataset}/arch_${architecture.arch}/compress_${compress}/opt_${optimizer.mode}/sched_${scheduler.name}/lr_${format:${optimizer.kwargs.lr},.1e}/wdec_${format:${optimizer.kwargs.weight_decay},.1e}/bs_${data_kwargs.batch_size}/seed_${seed}
seed: 123
timeout: 1440 # 24 hours
time: ${hydra:job.num}_${now:%Y-%m-%d_%H-%M-%S} # add job num because tiem is not when job runs

paths: 
  base_dir: ${hydra:runtime.cwd} 
  data: ${paths.base_dir}/data
  work: ${hydra.runtime.cwd}/outputs/${now:%Y-%m-%d_%H-%M-%S} # unfortunately cannot use hydra: in hydra so need to do everything by hand i.e. cannot use ${paths.base_dir}/outputs/{time}
  results: ${paths.base_dir}/results/${long_name}/jid_${job_id}
  logs: ${paths.base_dir}/logs/${long_name}/jid_${job_id}
  chckpnt: ${paths.base_dir}/checkpoints/${long_name}/jid_${job_id}
  pretrained: 
    save: ${paths.base_dir}/pretrained/${long_name}/jid_${job_id} # directory for saving pretrained models
    load: ${paths.base_dir}/pretrained/${long_name}/*  # directory for loading pretrained models if you use ** or * it will glob all matching files and take the latest
  
trainer: 
  max_epochs: 100
  progress_bar_refresh_rate: 0 
  gradient_clip_val: 3 
  resume_from_checkpoint: null # string to checkpoint if want to resume

  # DEEP LEARNING
  stochastic_weight_avg: False
  auto_lr_find: false # use  `optimizer.kwargs.lr` if want it

  # ENGINEERING / SPEED #
  gpus: 1 
  num_nodes: 1  # number gpu nodes
  precision: 16 # use 16 bit for speed 

  # DEBUGGING #
  fast_dev_run: false # use true to make a quick test (not full epoch)
  weights_summary: full # full to print show the entire model, top nly prints the top module
  track_grad_norm: -1 # use 2 to track L2 norms of grad


data_kwargs:
  num_workers: 16
  batch_size: 64
  val_split: 0.01
  pin_memory: True

optimizer: 
  name: ${optimizer.mode}_lr${optimizer.kwargs.lr}_w${optimizer.kwargs.lr}
  mode: ???
  kwargs:
    lr: 1e-3
    weight_decay: 1e-5

callbacks: # can use any callback name of lossyless.calllbacks, pl_bolts.callbacks, pl.callbacks
  is_force_no_additional_callback: false # force empty callback

  # all callback kwargs should be here (`is_use` just says whether to use that callback)
  LearningRateMonitor:
    is_use : true
    kwargs:
      logging_interval : epoch

########## HYDRA ##########
hydra:
  job:
    env_set:
      NCCL_DEBUG: INFO 

  run:
    dir: ${paths.work}

  sweep:
    dir:  ${paths.work}
    subdir: ${hydra.job.num}_${hydra.job.id}