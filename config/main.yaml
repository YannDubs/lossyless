defaults:

  # GENERAL #
  - _self_
  - user
  
  - logger: wandb
  - server: none
  - hypopt: none 
  - mode: none

  # FEATURIZER #
  - data@data_feat: mnist
  - architecture@encoder: resnet18
  - architecture@online_evaluator: mlp_probe
  - checkpoint@checkpoint_feat: bestValLoss # NB : will not be able to use Val loss for vib because validation is meaningless for distortion. Nor for INCE in the case where you don't use data augmentations at test time
  - optimizer@optimizer_feat: AdamW_lr1e-3_w1e-5 # larger learning rate gives smaller rate but smaller acc. should stick to using adam if not changing the  beta will effectively change the lr
  - scheduler@scheduler_feat: expdecay100 # expdecay also works well
  - optimizer@optimizer_coder: Adam_lr3e-4_w1e-5 # quite robust to this but lr shouldn't be to small
  - scheduler@scheduler_coder: expdecay100
  - optimizer@optimizer_online: SGD_lr3e-1_w1e-4
  - scheduler@scheduler_online: expdecay100 # ensure that can always keep up with the chaning representation
  - distortion: ivae
  - architecture@distortion.kwargs: none # just ensures that can change that from CLI
  - rate: H_hyper
  - finetune: none # change if using pretrained featurizer.
  - featurizer: neural_rec

  # PREDICTOR #
  - data@data_pred: data_feat # use same as data_feat
  - architecture@predictor: resnet18
  - checkpoint@checkpoint_pred: bestValLoss  # really important if using cosine sceduler
  - optimizer@optimizer_pred: Adam_lr1e-3_w1e-4
  - scheduler@scheduler_pred: cosine_restart

  # OVERIDES #
  - override hydra/job_logging: colorlog
  - override hydra/hydra_logging: colorlog


########## GENERAL ##########
experiment: ???
job_id: ??? # unique identifier
seed: 123
timeout: 1440 # 24 hours
time: ${hydra:job.num}_${now:%Y-%m-%d_%H-%M-%S} # add job num because tiem is not when job runs
long_name_feat: exp_${experiment}/datafeat_${data_feat.name}/feat_${featurizer.name}/dist_${distortion.name}/enc_${encoder.name}/rate_${rate.name}/optfeat_${optimizer_feat.name}/schedfeat_${scheduler_feat.name}/zdim_${encoder.z_dim}/zs_${featurizer.loss.n_z_samples}/beta_${format:${featurizer.loss.beta},.1e}/seed_${seed}/addfeat_${other.add_feat}
long_name_comm: ${long_name_feat}/datapred_${data_pred.name}
long_name_pred: ${long_name_comm}/optpred_${optimizer_pred.name}/schedpred_${scheduler_pred.name}/addpred_${other.add_pred}
is_only_feat: false # run rpredicotr?
is_return: false # will return everything (the data, the models, ...)
monitor_return: [] # normal return of the functio. THis is especially useful for hyperparameter tuning
monitor_direction: [] # whether the monitor should be maximized or minimized. usefule for hyperparameter tuning
is_no_save: ${is_return} # if you are sure you don't want to save anything

paths: #! the best practice is not to modify those paths but to simlink them to the places you want
  base_dir: ${hydra:runtime.cwd} 
  data: ${paths.base_dir}/data
  work: ${hydra.runtime.cwd}/outputs/${now:%Y-%m-%d_%H-%M-%S} # unfortunately cannot use hydra: in hydra so need to do everything by hand i.e. cannot use ${paths.base_dir}/outputs/{time}
  results: ${paths.base_dir}/results/${long_name}/jid_${job_id}
  logs: ${paths.base_dir}/logs/${long_name}/jid_${job_id}
  chckpnt: ${paths.base_dir}/checkpoints/${long_name}/jid_${job_id}
  pretrained: 
    save: ${paths.base_dir}/pretrained/${long_name}/jid_${job_id} # directory for saving pretrained models
    load: ${paths.base_dir}/pretrained/${long_name}/*  # directory for loading pretrained models if you use ** or * it will glob all matching files and take the latest

other: # some meta information that can be useful for internal stuff (usually dirty workarounds or for logging)
  is_debug: False # using debug mode
  is_quick: False # using a "quick" mode and should not log very slow things
  hydra_job_id: ${hydra:job.id} # this is the job id without the sweep number. Useful for filtering and grouping in wandb
  add_feat: null # some additional value for saving (e.g. current sweeping values)
  add_pred: null # some additional value for saving (e.g. current sweeping values)
  git_hash: null

### STAGE SPECIFIC ###
stage: ??? 
long_name: ???
checkpoint: {}
data: {}

### RUNNING ###
evaluation:
  is_eval_on_test: True # whether to evaluate on test. If not uses validation which is necessry if don't have access to test set
  is_est_entropies: False # whether to evaluate sample estimates of the entropies H[Y|Z], H[M(X)|Z], H[Z] should only use it with deterministic Z
  featurizer:
    ckpt_path: "best"
    is_evaluate: ${featurizer.is_train}
    is_online: True
  predictor:
    ckpt_path: "best"
    is_evaluate: ${predictor.is_train}
  communication:
    ckpt_path: ${paths.base_dir}/pretrained/${long_name_feat}/jid_${job_id}/best_featurizer.ckpt # uses the best from the featuization step
    is_evaluate: True

callbacks: # can use any callback name of lossyless.calllbacks, pl_bolts.callbacks, pl.callbacks
  is_force_no_additional_callback: false # force empty callback

  # all callback kwargs should be here (`is_use` just says whether to use that callback)
  LearningRateMonitor:
    is_use : true
    kwargs:
      logging_interval : epoch

trainer:
  #default_root_dir: ${paths.results}
  max_epochs: 200
  terminate_on_nan: false # makes it slower if true
  progress_bar_refresh_rate: 1000 # increase to show progress bar #TODO rm once prediction works
  resume_from_checkpoint: null # string to checkpoint if want to resume
  gradient_clip_val: 3 # 0 means no clip 
  reload_dataloaders_every_epoch: False
  log_every_n_steps: 500
  val_check_interval: 1.0 # decrease for subepoch checkpoitint

  # DEEP LEARNING
  stochastic_weight_avg: False

  # ENGINEERING / SPEED #
  gpus: 1 
  num_nodes: 1  # number gpu nodes
  precision: 16 # use 16 bit for speed 
  accumulate_grad_batches: 1
  sync_batchnorm: false # whether to synchronize batch norm over GPUs

  # DEBUGGING #
  fast_dev_run: false # use true to make a quick test (not full epoch)
  track_grad_norm: -1 # use 2 to track L2 norms of grad
  overfit_batches: 0.0 # use 0.01 to make sure you can overfit 1% of training data => training works
  weights_summary: full # full to print show the entire model, top nly prints the top module
  profiler: null # use `simple` or `"advanced"` to find bottleneck

########## FEATURIZER ##########
### DATA ###
data_feat: {}

### MODELS ###
encoder:
  name: ???
  z_dim: 128
  arch: ???
  arch_kwargs: {}
  fam_kwargs: {}

online_evaluator:
  name: ???
  arch: ???
  arch_kwargs: {}
  loss_kwargs: 
    is_classification: ${data.target_is_clf}

### OPTIMIZER ###
optimizer_feat: 
  name: ${optimizer_feat.mode}_lr${format:${optimizer_feat.kwargs.lr},.1e}_w${format:${optimizer_feat.kwargs.weight_decay},.1e}
  mode: ???
  kwargs:
    lr: ???
    weight_decay: 0
    is_lars: false # true can be useful if large batch sizes
scheduler_feat: {}

optimizer_coder: 
  name: ${optimizer_coder.mode}_lr${format:${optimizer_coder.kwargs.lr},.1e}_w${format:${optimizer_coder.kwargs.weight_decay},.1e}
  mode: ???
  kwargs:
    lr: ???
    weight_decay: 0
    is_lars: ${optimizer_feat.kwargs.is_lars}
scheduler_coder: {}

optimizer_online: 
  name: ${optimizer_online.mode}_lr${format:${optimizer_online.kwargs.lr},.1e}_w${format:${optimizer_online.kwargs.weight_decay},.1e}
  mode: ???
  kwargs:
    lr: ???
    weight_decay: 0
    is_lars: ${optimizer_feat.kwargs.is_lars}
scheduler_online: {}

### RUNNING ###
checkpoint_feat: {}
# dictionnary that will update the trainer (this is done because often the trainer is the same for feat and pred so want to minimize replication)
update_trainer_feat: {}

########## PREDICTOR ##########
### DATA ###
data_pred: {}

### MODEL ###
predictor:
  name: ???
  is_train: True
  arch: ???
  arch_kwargs: {}

### OPTIMIZER ###
optimizer_pred: 
  name: ${optimizer_pred.mode}_lr${format:${optimizer_pred.kwargs.lr},.1e}_w${format:${optimizer_pred.kwargs.weight_decay},.1e}
  mode: ???
  kwargs:
    lr: ???
    weight_decay: 0
scheduler_pred: {}

### RUNNING ###
checkpoint_pred: {}
# dictionnary that will update the trainer (this is done because often the trainer is the same for feat and pred so want to minimize replication)
update_trainer_pred: {}

########## HYDRA ##########
hydra:
  job:
    env_set:
      NCCL_DEBUG: INFO 

  run:
    dir: ${paths.work}

  sweep:
    dir:  ${paths.work}
    subdir: ${hydra.job.num}_${hydra.job.id}