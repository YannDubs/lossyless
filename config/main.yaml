defaults:
  - hydra/job_logging: colorlog
  - hydra/hydra_logging: colorlog
  
  - user
  - data: mnist
  - encoder: resnet
  - distortion: ivae
  - rate: MI_unitgaussian
  - server: none 
  # cannot use server directly (besides none) need the correct hydra/launcher: (i.e. submitit_slurm or submitit_local)


### GENERAL ###
experiment: ???
job_id: ??? # unique identifier
seed: 123
timeout: 1440 # 24 hours
time: ${hydra:job.num}_${now:%Y-%m-%d_%H-%M-%S} # add job num because tiem is not when job runs
long_name: exp_${experiment}/data_${data.name}/dist_${distortion.name}/enc_${encoder.name}/rate_${rate.name}/opt_${optimizer.name}/zdim_${encoder.z_dim}/zs_${loss.n_z_samples}/beta_${loss.beta}/seed_${seed}
is_train_compressor: True # whether to train the compressor, if not will load a pretrained comrpessor if it exists
is_train_predictor: True # whether to train the predictor if not will skip

paths:
  # the best practice is not to modify those paths but to simlink them to the places you want
  base_dir: ${hydra:runtime.cwd} 
  data: ${paths.base_dir}/data
  work: ${hydra.runtime.cwd}/outputs/${now:%Y-%m-%d_%H-%M-%S} # unfortunately cannot use hydra: in hydra so need to do everything by hand i.e. cannot use ${paths.base_dir}/outputs/{time}
  results: ${paths.base_dir}/results/${long_name}/jid_${job_id}
  logs: ${paths.base_dir}/logs/${long_name}/jid_${job_id}
  chckpnt: ${paths.base_dir}/checkpoints/jid_${job_id} 
  pretrained: 
    save: ${paths.base_dir}/pretrained/${long_name}/jid_${job_id} # directory for saving pretrained models
    load: ${paths.base_dir}/pretrained/${long_name}/*  # directory for loading pretrained models if you use ** or * it will glob all matching files and take the latest
  
  
evaluation:
  is_est_entropies: False # whether to evaluate sample estimates of the entropies H[Y|Z], H[M(X)|Z], H[Z] should only use it with deterministic Z
  is_reevaluate: False

other:
  # some meta information that can be useful for internal stuff (usually dirty workarounds or for logging)
  is_debug: False # using debug mode
  is_quick: False # using a "quick" mode and should not log very slow things
  hydra_job_id: ${hydra:job.id} # this is the job id without the sweep number. Useful for filtering and grouping in wandb

optimizer: # might need to be a group at some point
  name: adam # not used yet but can change if needed
  lr: 1e-3
  weight_decay: 0
  is_lars: false # whether to use LARS optimizer, useful for large batches.
  scheduler:
    name: "expdecay"
    decay_factor: 100 # by how much to reduce lr during training

# only used if coder needs an optimizer (for coding)
optimizer_coder :
  name: adam # not used yet but can change if needed
  lr: 0.003 
  scheduler:
    decay_factor: 100
    name: "expdecay"

logger:
  name: wandb # select which one to use (false for nothing)

  csv:
    save_dir: ${paths.logs}
    name: ${job_id}

  wandb:
    name: ${job_id}
    project: lossyless
    entity: ${wandb_entity}
    group: ${experiment}
    offline: false # Run offline (data can be streamed later to wandb servers).
    reinit: False # ensure that same wandb after preemption
    id: ${job_id}
    save_dir: ${paths.logs}

  tensorboard:
    save_dir: ${paths.logs}
    name: ${job_id}

callbacks:
  additional: []

  ModelCheckpoint_compressor:
    dirpath: ${paths.chckpnt} 
    monitor: train/loss
    mode: "min"
    verbose: true
    save_last: True
    save_top_k: 1
    save_weights_only: false

trainer:
  #default_root_dir: ${paths.results}
  max_epochs: 200
  terminate_on_nan: true
  progress_bar_refresh_rate: 0 # increase to show progress bar
  resume_from_checkpoint: null # string to checkpoint if want to resume
  gradient_clip_val: 0 # increase to clip grad
  reload_dataloaders_every_epoch: False
  log_every_n_steps: 100
  
  # ENGINEERING / SPEED
  gpus: 1 
  num_nodes: 1  # number gpu nodes
  precision: 16 # use 16 bit for speed 

  # DEBUGGING
  fast_dev_run: false # use true to make a quick test (not full epoch)
  track_grad_norm: -1 # use 2 to track L2 norms of grad
  overfit_batches: 0.0 # use 0.01 to make sure you can overfit 1% of training data => training works
  weights_summary: full # full to print show the entire model, top nly prints the top module
  profiler: null # use `simple` or `"advanced"` to find bottleneck

### GROUP DEFAULTS ###
data:
  name: ???
  dataset: ???
  mode: ???
  length: ???
  shape: ???
  target_shape: ???
  aux_shape: ???
  aux_is_clf: ??? 
  target_is_clf: ???
  neg_factor: ??? # useful for contrastive loss. should be len(train_dataset) / (2*batch_size-1)

  kwargs:
    data_dir: ${paths.data}
    batch_size: 128
    reload_dataloaders_every_epoch: ${trainer.reload_dataloaders_every_epoch}

encoder:
  name: ???
  z_dim: 10
  arch: ???
  arch_kwargs:
    complexity: ???
  fam: diaggaussian
  fam_kwargs: {}

distortion:
  name: ???
  mode: ???
  factor_beta : 1 # factor that multiplies beta 
  kwargs: {}

rate:
  name: ???
  range_coder: null
  factor_beta : 1 # factor that multiplies beta 
  kwargs: {}

loss:
  n_z_samples: 1 # number of samples tu use inside log (like IWAE)
  beta: 1

### HYDRA ###
hydra:
  job:
    env_set:
      NCCL_DEBUG: INFO 

  run:
    dir: ${paths.work}

  sweep:
    dir:  ${paths.work}
    subdir: ${hydra.job.num}_${hydra.job.id}
