# @package _global_

# test whether can overfit training

timeout: 60

trainer:
  track_grad_norm: 2 # use 2 to track L2 norms of grad
  overfit_batches: 0.01 # use 0.01 to make sure you can overfit 1% of training data => training works
  max_epochs: 100
  log_every_n_steps: 20

logger:
  wandb:
    tags: [test]
    anonymous: true

callbacks:
  additional: ["GPUStatsMonitor","TrainingDataMonitor","ModuleDataMonitor"] # if add other callbacks through additional will be replaced

  TrainingDataMonitor:
    log_every_n_steps: 50

  ModuleDataMonitor:
    log_every_n_steps: 50
    submodules: ["p_ZlX.mapper"]
  