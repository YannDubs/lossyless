{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Hub.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOznPPXW0fMuw7X86xiXCZE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "cebc02359e9146aab6e2b7411d17baf1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_1a8e152642f0437d802f7a74b01d4179",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_db6e6f3fdbb34cdfbedcbfcceaf04f00",
              "IPY_MODEL_8d3b9aeeb9384f80a1a6d8a688e5ad59"
            ]
          }
        },
        "1a8e152642f0437d802f7a74b01d4179": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "db6e6f3fdbb34cdfbedcbfcceaf04f00": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_bc3cda7ea0e444c2a19facddb92e0fda",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 2640397119,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 2640397119,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e1ab99f187a14cd6a89ea91ec5c01213"
          }
        },
        "8d3b9aeeb9384f80a1a6d8a688e5ad59": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_9373f0aa9ae140f8b455c98ec2cca4cf",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 2640397312/? [02:49&lt;00:00, 15578595.28it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d5946ea3d9d8496c8826a9a7ca769625"
          }
        },
        "bc3cda7ea0e444c2a19facddb92e0fda": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e1ab99f187a14cd6a89ea91ec5c01213": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9373f0aa9ae140f8b455c98ec2cca4cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d5946ea3d9d8496c8826a9a7ca769625": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YannDubs/lossyless/blob/main/Hub.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mrrACM-9XKC9"
      },
      "source": [
        "# Using lossyless CLIP compressor\n",
        "\n",
        "This notebook contains a minimal example for using the CLIP compressor pretrained on pytorch Hub in our paper [**Lossy Compression for Lossless Prediction**](https://arxiv.org/pdf/2106.10800.pdf). \n",
        "\n",
        "**Make sure that you use a GPU** (on COLAB: runtime -> change runtime type -> Hardware accelerator: GPU)\n",
        "\n",
        "## Environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XrRKIg_SW24M",
        "outputId": "304eccd0-8f04-413a-df61-60235509e3fe"
      },
      "source": [
        "!pip install torch torchvision tqdm numpy compressai scikit-learn git+https://github.com/openai/CLIP.git --quiet"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[?25l\r\u001b[K     |█▍                              | 10kB 25.1MB/s eta 0:00:01\r\u001b[K     |██▉                             | 20kB 18.7MB/s eta 0:00:01\r\u001b[K     |████▎                           | 30kB 15.1MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 40kB 13.8MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 51kB 8.5MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 61kB 7.9MB/s eta 0:00:01\r\u001b[K     |██████████                      | 71kB 8.9MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 81kB 9.4MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 92kB 9.8MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 102kB 8.2MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 112kB 8.2MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 122kB 8.2MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 133kB 8.2MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 143kB 8.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 153kB 8.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 163kB 8.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 174kB 8.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 184kB 8.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 194kB 8.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 204kB 8.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 215kB 8.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 225kB 8.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 235kB 8.2MB/s \n",
            "\u001b[?25h\u001b[?25l\r\u001b[K     |█████                           | 10kB 36.2MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 20kB 40.3MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 30kB 45.5MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 40kB 43.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 51kB 28.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 61kB 31.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 71kB 10.3MB/s \n",
            "\u001b[?25h  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for ftfy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: clip 1.0 has requirement torch~=1.7.1, but you'll have torch 1.9.0+cu102 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: clip 1.0 has requirement torchvision~=0.8.2, but you'll have torchvision 0.10.0+cu102 which is incompatible.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3WkrAFqjYJra"
      },
      "source": [
        "## Downloading the pretrained compressor\n",
        "\n",
        "First we will download the compressor. The following command returns the compressor as well as the transform that should be applied to the images before compression. The transformation resizes+crops images to `(3,224,224)`, applies CLIP normalization, and converts to tensor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zs9RI7DCX6Ug",
        "outputId": "ade465a1-c9cf-4aa8-f9d3-40152fa91861"
      },
      "source": [
        "# Load the desired compressor and transformation to apply to images (by default on GPU if available)\n",
        "compressor, transform = torch.hub.load(\n",
        "    \"YannDubs/lossyless:main\", \"clip_compressor_b005\"\n",
        ")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://github.com/YannDubs/lossyless/archive/main.zip\" to /root/.cache/torch/hub/main.zip\n",
            "Downloading: \"https://github.com/YannDubs/lossyless/releases/download/v0.1-alpha/beta5e-02_factorized_rate.pt\" to /root/.cache/torch/hub/checkpoints/beta5e-02_factorized_rate.pt\n",
            "100%|███████████████████████████████████████| 354M/354M [00:10<00:00, 33.8MiB/s]\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Im5mOcfRZ57N"
      },
      "source": [
        "You can also use stronger compressor or less strong compressor. Specifically, `b005` stands for $\\beta=0.05$ and you can increase $\\beta$ to increase compression power (this is actually $\\frac{1}{\\beta}$ in the paper :/ ). To see avaliable compressors use the following command:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xpfeWVB5Y3y6",
        "outputId": "1077d22d-83cc-4bd8-f765-d3b69dab6fc9"
      },
      "source": [
        "# list available compressors. b01 compresses the most (b01 > b005 > b001)\n",
        "torch.hub.list(\"YannDubs/lossyless:main\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using cache found in /root/.cache/torch/hub/YannDubs_lossyless_main\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['clip_compressor_b001', 'clip_compressor_b005', 'clip_compressor_b01']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hFIAIh-OalZr"
      },
      "source": [
        "## Compressing an entire dataset\n",
        "\n",
        "Let's see how to compress and save a torchvision dataset to file. We will use STL10 as it is quick and easy to download.\n",
        "\n",
        "Importantly we will use `transform` on each image \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 117,
          "referenced_widgets": [
            "cebc02359e9146aab6e2b7411d17baf1",
            "1a8e152642f0437d802f7a74b01d4179",
            "db6e6f3fdbb34cdfbedcbfcceaf04f00",
            "8d3b9aeeb9384f80a1a6d8a688e5ad59",
            "bc3cda7ea0e444c2a19facddb92e0fda",
            "e1ab99f187a14cd6a89ea91ec5c01213",
            "9373f0aa9ae140f8b455c98ec2cca4cf",
            "d5946ea3d9d8496c8826a9a7ca769625"
          ]
        },
        "id": "rKy2AszZbDqR",
        "outputId": "b919d6b4-b569-40a4-b86f-69ea52ea1cc7"
      },
      "source": [
        "from torchvision.datasets import STL10\n",
        "DATA_DIR = \"data/\"\n",
        "\n",
        "# Load some data to compress and apply transformation\n",
        "stl10_train = STL10(DATA_DIR, download=True, split=\"train\", transform=transform)\n",
        "stl10_test = STL10(DATA_DIR, download=True, split=\"test\", transform=transform)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading http://ai.stanford.edu/~acoates/stl10/stl10_binary.tar.gz to data/stl10_binary.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cebc02359e9146aab6e2b7411d17baf1",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=2640397119.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Extracting data/stl10_binary.tar.gz to data/\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ybloItmrbh8X"
      },
      "source": [
        "Let us now compress the entire dataset and save it to file. We provide a helper function for that `compress_dataset` (see docstring for more information). This requires a GPU."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8WvvlCeHa3WZ",
        "outputId": "180cf182-c1b7-47df-8afc-04b36ab60379"
      },
      "source": [
        "# Rate: 1506.50 bits/img | Encoding: 347.82 img/sec\n",
        "compressor.compress_dataset(\n",
        "    stl10_train,\n",
        "    f\"{DATA_DIR}/stl10_train_Z.bin\",\n",
        "    label_file=f\"{DATA_DIR}/stl10_train_Y.npy\",\n",
        ")\n",
        "compressor.compress_dataset(\n",
        "    stl10_test,\n",
        "    f\"{DATA_DIR}/stl10_test_Z.bin\",\n",
        "    label_file=f\"{DATA_DIR}/stl10_test_Y.npy\",\n",
        ")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "100%|██████████| 40/40 [00:18<00:00,  2.18it/s]\n",
            "  0%|          | 0/63 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Rate: 1506.62 bits/img | Encoding: 271.71 img/sec \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 63/63 [00:26<00:00,  2.37it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Rate: 1507.56 bits/img | Encoding: 301.09 img/sec \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfJfGq8icLX-"
      },
      "source": [
        "The dataset is now saved to file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zsxKPXXpcJ5t",
        "outputId": "57268e7b-d662-4482-ca25-36792adfe803"
      },
      "source": [
        "!du -sh data/stl10_train_Z.bin"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "920K\tdata/stl10_train_Z.bin\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u9Tblybacw7r"
      },
      "source": [
        "Let us now load and decompress the dataset from file. The decompressed data is loaded as numpy array. This does not use a GPU by default. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fKFGAnFRcZQc",
        "outputId": "11840051-b93b-4bad-9435-64d30c698a4e"
      },
      "source": [
        "# Decoding: 1062.38 img/sec\n",
        "Z_train, Y_train = compressor.decompress_dataset(\n",
        "    f\"{DATA_DIR}/stl10_train_Z.bin\", label_file=f\"{DATA_DIR}/stl10_train_Y.npy\"\n",
        ")\n",
        "Z_test, Y_test = compressor.decompress_dataset(\n",
        "    f\"{DATA_DIR}/stl10_test_Z.bin\", label_file=f\"{DATA_DIR}/stl10_test_Y.npy\"\n",
        ")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 5000/5000 [00:04<00:00, 1090.42it/s]\n",
            "  0%|          | 0/8000 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Decoding: 1086.62 img/sec \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 8000/8000 [00:07<00:00, 1104.77it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Decoding: 1101.41 img/sec \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDuvTUx3dTMn"
      },
      "source": [
        "Now that we have the decompressed data, let's test how well we can classify from it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LQr94nccdRjJ",
        "outputId": "8746354d-ccea-4f96-babe-7131cb8700f2"
      },
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "import time\n",
        "\n",
        "# Accuracy: 98.65% | Training time: 0.5 sec\n",
        "clf = LinearSVC(C=7e-3)\n",
        "start = time.time()\n",
        "clf.fit(Z_train, Y_train)\n",
        "delta_time = time.time() - start\n",
        "acc = clf.score(Z_test, Y_test)\n",
        "print(\n",
        "    f\"Downstream STL10 accuracy: {acc*100:.2f}%.  \\t Training time: {delta_time:.1f} \"\n",
        ")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downstream STL10 accuracy: 98.64%.  \t Training time: 0.6 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ozBBPMrKemWd"
      },
      "source": [
        "## Representing a batch of image\n",
        "\n",
        "In case you have a batch of images and you only want to represent them (skip the compression / decompression steps). Then you can do the following."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c4FwBa_0eQIb",
        "outputId": "f6161ece-249c-4aae-d6a0-1e493e467b19"
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# 1. Get a batch of images from STL10 (note that correct transform already applied)\n",
        "for X, _ in DataLoader(stl10_train, batch_size=128):\n",
        "  break\n",
        "print(\"X shape:\", X.shape)\n",
        "\n",
        "# 2. Transfer batch to CUDA and half precision\n",
        "X = X.to(\"cuda\").half()\n",
        "\n",
        "# 3. Represent the data (equivalent of compression + decompressing but quicker)\n",
        "Z = compressor(X)\n",
        "print(\"Z shape:\", Z.shape)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X shape: torch.Size([128, 3, 224, 224])\n",
            "Z shape: torch.Size([128, 512])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zre4uvGzfaV8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}