# @package _global_
job_id: ${hydra:job.num}_${hydra:job.id} # job num is number in sweep. Id is unique ID like SLURM_JOB_ID


hydra:
  launcher:
    # maximum time for the job in minutes
    timeout_min: ${timeout}
    # number of cpus to use for each task
    cpus_per_task: 10
    # number of gpus to use on each node
    gpus_per_node: ${trainer.gpus}
    # number of tasks to spawn on each node
    tasks_per_node: 1 # number of tasks on single machine
    # memory to reserve for the job on each node (in GB)
    mem_gb: 32
    # number of nodes to use for the job
    nodes: ${trainer.num_nodes}
    # name of the job
    name: ${experiment}

    # slurm partition to use on the cluster
    partition: learnfair # scavenge, dev, priority, learnfair
    comment: null # you  have to add a comment when usintg the priority partition
    constraint: null # pascal | volta | volta32gb
    exclude: null
    array_parallelism: 1000
    
    max_num_timeout: 3 # alow resume from checkpointing
    additional_parameters: {} # any other parameters, Eg: {"mail-user": "blublu@fb.com", "mail-type": "BEGIN"}

  run:
    # unfortunately cannot use hydra: is hydra so need to do everything by hand
    # i.e. cannot use ${paths.work}
    dir: /checkpoint/${user}/${now:%Y-%m-%d_%H-%M-%S}

  sweep:
    dir:  /checkpoint/${user}/${now:%Y-%m-%d_%H-%M-%S}
    subdir: ${hydra.job.num}_${hydra.job.id}